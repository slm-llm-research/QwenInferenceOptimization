# Quick Start Guide

## ğŸš€ Get Started in 5 Minutes

This course teaches you LLM inference optimization and deployment from scratch. Here's how to begin:

### Step 1: Prerequisites Check

You'll need:
- **GPU**: NVIDIA GPU with CUDA 12.1+ (16GB+ VRAM recommended)
- **OS**: Linux (Ubuntu 20.04+) or macOS for development
- **Python**: 3.9, 3.10, or 3.11
- **Accounts** (for deployment weeks):
  - Runpod account (Week 6)
  - AWS account (Week 6-7)

### Step 2: Clone or Navigate to This Repository

```bash
cd /path/to/InferenceOptimization
```

### Step 3: Start with Week 1

```bash
cd week1-setup
cat README.md  # Read the detailed instructions
```

Each week's README contains:
- âœ… Clear learning objectives
- ğŸ“š Background concepts explained
- ğŸš€ Step-by-step instructions
- ğŸ› Troubleshooting guide
- âœ… Completion checklist

### Step 4: Follow Week by Week

**Week-by-week progression:**

1. **Week 1** (2-3 hours): Environment setup and first inference
2. **Week 2** (3-4 hours): Performance profiling and baseline metrics
3. **Week 3** (4-5 hours): GPU optimization and tuning
4. **Week 4** (1-2 hours): Integration week - learning and documentation
5. **Week 5** (3-4 hours): Multi-GPU distributed inference
6. **Week 6** (4-6 hours): Cloud deployment (Runpod + AWS EKS)
7. **Week 7** (2-3 hours): Load testing and final teardown

**Total time**: ~20-30 hours

## ğŸ’¡ Key Features

### Educational Focus
- **No prior vLLM or Kubernetes knowledge required**
- Clear explanations of every concept
- Hands-on code for every week
- Real-world deployment patterns

### Cost-Conscious
- âš ï¸ Detailed cost estimates for cloud resources
- ğŸ›‘ Comprehensive teardown instructions
- âœ… Verification scripts to avoid lingering charges
- ğŸ’° Cost management best practices

### Production-Ready
- Docker containerization
- Kubernetes deployment
- Load testing at scale
- Monitoring and optimization

## ğŸ“ Project Structure

```
InferenceOptimization/
â”œâ”€â”€ README.md                    # Course overview
â”œâ”€â”€ QUICKSTART.md               # This file
â”œâ”€â”€ requirements.txt            # All Python dependencies
â”‚
â”œâ”€â”€ week1-setup/                # Environment preparation
â”‚   â”œâ”€â”€ README.md              # Detailed week 1 guide
â”‚   â”œâ”€â”€ baseline_inference.py  # First inference test
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ week2-profiling/            # Performance benchmarking
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ benchmark_latency.py
â”‚   â”œâ”€â”€ benchmark_throughput.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ week3-optimization/         # GPU tuning
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ optimize_memory_utilization.py
â”‚   â”œâ”€â”€ optimize_max_num_seqs.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ week4-integration/          # Learning week
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ week5-distributed/          # Multi-GPU inference
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ test_tensor_parallel.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ week6-deployment/           # Cloud deployment
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ deploy_eks.sh          # EKS deployment script
â”‚   â”œâ”€â”€ teardown_eks.sh        # EKS teardown script âš ï¸
â”‚   â”œâ”€â”€ docker/                # Container files
â”‚   â””â”€â”€ kubernetes/            # K8s manifests
â”‚
â””â”€â”€ week7-load-testing/         # Production testing
    â”œâ”€â”€ README.md
    â”œâ”€â”€ run_load_test.py       # Load testing
    â”œâ”€â”€ verify_cleanup.sh      # Cleanup verification âš ï¸
    â””â”€â”€ ...
```

## âš ï¸ Important Notes

### About Cloud Costs

**Weeks 6-7 use cloud resources that cost money!**

- Always read the cost warnings in each week's README
- Follow teardown instructions immediately after testing
- Use the verification scripts to ensure cleanup
- Set up billing alerts on AWS

### About Multi-GPU (Week 5)

Week 5 requires multi-GPU access. Options:
1. Rent multi-GPU instance (Runpod, AWS)
2. Skip and proceed to Week 6 (single-GPU deployment works)
3. Review concepts without running code

### Recommended Hardware

**For local development (Week 1-4)**:
- GPU: RTX 3090/4090, A100, A10G, or T4
- VRAM: 16GB minimum, 24GB+ recommended
- RAM: 16GB+

**For deployment (Week 6-7)**:
- Use cloud GPUs (cheaper than buying hardware)
- AWS g5.xlarge or Runpod A10G recommended

## ğŸ†˜ Getting Help

### If You Get Stuck

1. **Check the week's README** - Most issues are covered in troubleshooting sections
2. **Review prerequisites** - Ensure all tools are installed correctly
3. **Check the specific error** - Most errors have clear solutions in the READMEs

### Common Issues

**"CUDA out of memory"**
- Expected during optimization experiments
- Helps find the limits
- Reduce batch size or max_tokens

**"No module named 'vllm'"**
- Virtual environment not activated
- Run: `source vllm-env/bin/activate`

**"Cannot connect to cluster"**
- Update kubeconfig: `aws eks update-kubeconfig --name CLUSTER --region REGION`

## ğŸ“š What You'll Learn

By the end of this course:

âœ… LLM inference optimization techniques
âœ… GPU performance profiling and tuning
âœ… Distributed inference patterns
âœ… Docker and Kubernetes for ML
âœ… Cloud deployment (AWS EKS)
âœ… Production load testing
âœ… Cost optimization strategies

## ğŸ¯ Success Criteria

You'll know you've succeeded when you can:

1. Deploy a production-ready LLM inference endpoint
2. Optimize GPU utilization from ~60% to 90%+
3. Scale inference across multiple GPUs
4. Handle 100+ concurrent users
5. Properly manage cloud resources and costs

## ğŸ”œ Ready to Start?

```bash
cd week1-setup
cat README.md
```

Good luck on your LLM inference optimization journey! ğŸš€

---

**Remember**: This is a hands-on learning course. Don't just read - run the code, experiment with parameters, and learn by doing!

