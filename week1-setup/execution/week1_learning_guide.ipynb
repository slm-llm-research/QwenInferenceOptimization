{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1: vLLM & Qwen2.5 - Complete Learning Guide\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "1. **Set up and verify** your vLLM environment with GPU support\n",
        "2. **Download and cache** the Qwen2.5-7B-Instruct model from HuggingFace\n",
        "3. **Perform basic inference** using vLLM's LLM class\n",
        "4. **Understand sampling parameters** and how they affect text generation\n",
        "5. **Experiment systematically** with different parameter combinations\n",
        "6. **Analyze results** to determine best practices for different use cases\n",
        "\n",
        "## üìö Structure\n",
        "\n",
        "This notebook covers 6 main sections (matching the week1-setup scripts):\n",
        "1. **Environment Verification** (`verify_environment.py`)\n",
        "2. **Model Download** (`download_model.py`)\n",
        "3. **Baseline Inference** (`baseline_inference.py`)\n",
        "4. **Sampling Parameters** (`experiment_sampling_params.py`)\n",
        "5. **Results Comparison** (`compare_results.py`)\n",
        "6. **Custom Testing** (`test_custom_params.py`)\n",
        "\n",
        "## üö® Learning Approach\n",
        "\n",
        "- **Learn by implementing**: Don't copy from the original scripts!\n",
        "- **Follow step-by-step guides**: Each function has detailed instructions\n",
        "- **Test incrementally**: Run cells as you complete them\n",
        "- **Check expected outputs**: Verify your implementation\n",
        "- **Learn from pitfalls**: Common mistakes are highlighted\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Environment Verification\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- Programmatically check Python version\n",
        "- Verify PyTorch and CUDA installation\n",
        "- Inspect GPU properties using PyTorch\n",
        "- Check required libraries (vLLM, HuggingFace Hub)\n",
        "- Create comprehensive validation reports\n",
        "\n",
        "## üìñ Background\n",
        "Before running LLM inference, ensure:\n",
        "- Python 3.9+ is installed\n",
        "- PyTorch with CUDA support\n",
        "- CUDA can access your GPU(s)\n",
        "- vLLM and HuggingFace Hub are available\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.1: `check_python_version()`\n",
        "\n",
        "### üéØ Goal\n",
        "Create a function that checks if Python version is 3.9 or higher.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Import the `sys` module at the top of your notebook\n",
        "- Hint: `sys` is a built-in module, no installation needed\n",
        "\n",
        "**Step 2:** Inside the function, access the Python version information\n",
        "- Use `sys.version_info` to get version details\n",
        "- This returns an object with attributes: `major`, `minor`, `micro`\n",
        "\n",
        "**Step 3:** Print a header message\n",
        "- Use an emoji like üîç to make it visually clear\n",
        "- Print \"Checking Python version...\"\n",
        "\n",
        "**Step 4:** Print the current Python version\n",
        "- Format: \"Python version: {major}.{minor}.{micro}\"\n",
        "- Use f-strings for formatting\n",
        "\n",
        "**Step 5:** Check if version meets requirements\n",
        "- Requirement: Python 3.9 or higher\n",
        "- Check: `version.major == 3 and version.minor >= 9`\n",
        "- Why both conditions? Python 4.x would also pass if it existed!\n",
        "\n",
        "**Step 6:** Print success or failure message\n",
        "- If compatible: Print \"‚úì Python version is compatible (3.9+)\"\n",
        "- If not: Print \"‚úó Python version must be 3.9 or higher\"\n",
        "- Use proper indentation (3 spaces) to align with other output\n",
        "\n",
        "**Step 7:** Return the result\n",
        "- Return `True` if compatible\n",
        "- Return `False` if not compatible\n",
        "\n",
        "### ‚úÖ Expected Output\n",
        "```\n",
        "üîç Checking Python version...\n",
        "   Python version: 3.10.12\n",
        "   ‚úì Python version is compatible (3.9+)\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Forgetting to check major version**: Just checking `minor >= 9` would fail for Python 2.9 (if it existed)\n",
        "2. **Wrong attribute names**: It's `version_info.major`, not `version_info.version`\n",
        "3. **Not returning a boolean**: The function should return True/False for later use\n",
        "\n",
        "### üß™ Test Your Function\n",
        "After implementing, run:\n",
        "```python\n",
        "result = check_python_version()\n",
        "assert isinstance(result, bool), \"Function should return a boolean\"\n",
        "print(f\"\\nFunction returned: {result}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import necessary modules\n",
        "import sys\n",
        "\n",
        "# TODO: Implement check_python_version() function\n",
        "def check_python_version():\n",
        "    \"\"\"Check Python version is 3.9+\"\"\"\n",
        "    # Step 1: Access version info\n",
        "    \n",
        "    # Step 2: Print header\n",
        "    \n",
        "    # Step 3: Print current version\n",
        "    \n",
        "    # Step 4: Check requirements\n",
        "    \n",
        "    # Step 5: Print result message\n",
        "    \n",
        "    # Step 6: Return boolean\n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# result = check_python_version()\n",
        "# assert isinstance(result, bool)\n",
        "# print(f\"\\nTest passed! Result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.2: `check_pytorch()`\n",
        "\n",
        "### üéØ Goal\n",
        "Check if PyTorch is installed and display its version.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Print a header with proper spacing\n",
        "- Use `\\n` before the emoji for spacing\n",
        "- Print \"üîç Checking PyTorch...\"\n",
        "\n",
        "**Step 2:** Try to import PyTorch\n",
        "- Use a try-except block to handle ImportError\n",
        "- Try: `import torch`\n",
        "- This is the safe way to check if a library is installed\n",
        "\n",
        "**Step 3:** If import succeeds, get version info\n",
        "- Access `torch.__version__` to get version string\n",
        "- Print: \"PyTorch version: {version}\"\n",
        "- Print: \"‚úì PyTorch is installed\"\n",
        "- Return `True`\n",
        "\n",
        "**Step 4:** If import fails, provide helpful guidance\n",
        "- Catch `ImportError`\n",
        "- Print: \"‚úó PyTorch is not installed\"\n",
        "- Print installation command: \"Run: pip install torch --index-url https://download.pytorch.org/whl/cu121\"\n",
        "- Return `False`\n",
        "\n",
        "### ‚úÖ Expected Output (if PyTorch installed)\n",
        "```\n",
        "üîç Checking PyTorch...\n",
        "   PyTorch version: 2.1.0+cu121\n",
        "   ‚úì PyTorch is installed\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Importing at module level**: Import inside the function so ImportError can be caught\n",
        "2. **Not providing installation instructions**: Users need to know how to fix the issue\n",
        "3. **Forgetting indentation**: Error messages should be indented with 3 spaces for alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement check_pytorch() function\n",
        "def check_pytorch():\n",
        "    \"\"\"Check PyTorch installation\"\"\"\n",
        "    # Step 1: Print header\n",
        "    \n",
        "    # Step 2: Try to import torch\n",
        "    \n",
        "    # Step 3: If success, get and print version\n",
        "    \n",
        "    # Step 4: If fail, print error and instructions\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# result = check_pytorch()\n",
        "# print(f\"\\nPyTorch available: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.3: `check_cuda()`\n",
        "\n",
        "### üéØ Goal\n",
        "Check CUDA availability and display GPU information.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Print the header\n",
        "- Print \"\\nüîç Checking CUDA...\"\n",
        "\n",
        "**Step 2:** Wrap everything in a try-except block\n",
        "- Catch general `Exception` to handle any unexpected errors\n",
        "\n",
        "**Step 3:** Import torch (inside the try block)\n",
        "- `import torch`\n",
        "\n",
        "**Step 4:** Check if CUDA is available\n",
        "- Use `torch.cuda.is_available()` - returns True/False\n",
        "- Store result in a variable\n",
        "\n",
        "**Step 5:** If CUDA is available, gather detailed information\n",
        "- Print: \"CUDA available: True\"\n",
        "- Get CUDA version: `torch.version.cuda`\n",
        "- Print: \"CUDA version: {version}\"\n",
        "- Get number of GPUs: `torch.cuda.device_count()`\n",
        "- Print: \"Number of GPUs: {count}\"\n",
        "\n",
        "**Step 6:** Loop through each GPU and get details\n",
        "- Use `for i in range(torch.cuda.device_count()):`\n",
        "- Get GPU name: `torch.cuda.get_device_name(i)`\n",
        "- Get GPU memory: `torch.cuda.get_device_properties(i).total_memory`\n",
        "- Convert memory from bytes to GB: divide by `1024**3`\n",
        "- Print: \"GPU {i}: {name} ({memory:.1f} GB)\"\n",
        "\n",
        "**Step 7:** Print success message and return True\n",
        "- Print: \"‚úì CUDA is available and working\"\n",
        "- Return `True`\n",
        "\n",
        "**Step 8:** If CUDA is not available (in the else block)\n",
        "- Print: \"‚úó CUDA is not available\"\n",
        "- Print helpful note about CPU fallback\n",
        "- Return `False`\n",
        "\n",
        "**Step 9:** Handle exceptions in the except block\n",
        "- Print: \"‚úó Error checking CUDA: {error_message}\"\n",
        "- Return `False`\n",
        "\n",
        "### ‚úÖ Expected Output (with GPU)\n",
        "```\n",
        "üîç Checking CUDA...\n",
        "   CUDA available: True\n",
        "   CUDA version: 12.1\n",
        "   Number of GPUs: 1\n",
        "   GPU 0: NVIDIA A100-SXM4-40GB (40.0 GB)\n",
        "   ‚úì CUDA is available and working\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Memory unit conversion**: `total_memory` is in bytes, need to divide by `1024**3` for GB\n",
        "2. **Forgetting the .1f format**: Shows memory with 1 decimal place\n",
        "3. **Not handling the case when device_count is 0**: The for loop handles this automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement check_cuda() function\n",
        "def check_cuda():\n",
        "    \"\"\"Check CUDA availability\"\"\"\n",
        "    # Step 1: Print header\n",
        "    \n",
        "    # Step 2: Start try-except block\n",
        "    \n",
        "    # Step 3: Import torch\n",
        "    \n",
        "    # Step 4: Check CUDA availability\n",
        "    \n",
        "    # Step 5: If available, get details\n",
        "    \n",
        "    # Step 6: Loop through GPUs\n",
        "    \n",
        "    # Step 7: Print success and return True\n",
        "    \n",
        "    # Step 8: Handle not available case\n",
        "    \n",
        "    # Step 9: Handle exceptions\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# result = check_cuda()\n",
        "# print(f\"\\nCUDA available: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.4: `check_vllm()`\n",
        "\n",
        "### üéØ Goal\n",
        "Check if vLLM library is installed.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Follow the same pattern as `check_pytorch()`\n",
        "- Print header: \"\\nüîç Checking vLLM...\"\n",
        "\n",
        "**Step 2:** Use try-except for import\n",
        "- Try to `import vllm`\n",
        "- Get version: `vllm.__version__`\n",
        "- Print version and success message\n",
        "- Return `True`\n",
        "\n",
        "**Step 3:** Handle ImportError\n",
        "- Print error and installation command: \"Run: pip install vllm\"\n",
        "- Return `False`\n",
        "\n",
        "### ‚úÖ Expected Output\n",
        "```\n",
        "üîç Checking vLLM...\n",
        "   vLLM version: X.X.X\n",
        "   ‚úì vLLM is installed\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement check_vllm() function\n",
        "def check_vllm():\n",
        "    \"\"\"Check vLLM installation\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# result = check_vllm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.5: `check_huggingfacehub()`\n",
        "\n",
        "### üéØ Goal\n",
        "Check if HuggingFace Hub library is installed.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Follow the same pattern as `check_pytorch()`\n",
        "- Print header: \"\\nüîç Checking HuggingFace Hub...\"\n",
        "\n",
        "**Step 2:** Use try-except for import\n",
        "- Try to `import huggingface_hub`\n",
        "- Get version: `huggingface_hub.__version__`\n",
        "- Print version and success message\n",
        "- Return `True`\n",
        "\n",
        "**Step 3:** Handle ImportError\n",
        "- Print error and installation command: \"Run: pip install huggingface-hub\"\n",
        "- Return `False`\n",
        "\n",
        "### ‚úÖ Expected Output\n",
        "```\n",
        "üîç Checking HuggingFace Hub...\n",
        "   HuggingFace Hub version: X.X.X\n",
        "   ‚úì HuggingFace Hub is installed\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement check_huggingfacehub() function\n",
        "def check_huggingfacehub():\n",
        "    \"\"\"Check HuggingFace Hub installation\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# result = check_huggingfacehub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1.6: `print_summary(results)`\n",
        "\n",
        "### üéØ Goal\n",
        "Create a summary of all environment checks.\n",
        "\n",
        "### üìù Function Signature\n",
        "```python\n",
        "def print_summary(results):\n",
        "    \"\"\"Print summary of all checks\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary with check names as keys and boolean results as values\n",
        "        \n",
        "    Returns:\n",
        "        bool: True if all checks passed, False otherwise\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Print a formatted header\n",
        "- Print a blank line\n",
        "- Print \"=\"*60 (creates a line of 60 equal signs)\n",
        "- Print \"SUMMARY\"\n",
        "- Print \"=\"*60 again\n",
        "\n",
        "**Step 2:** Determine if all checks passed\n",
        "- Use `all(results.values())` to check if all values are True\n",
        "- Store in variable `all_passed`\n",
        "\n",
        "**Step 3:** Loop through results and print each one\n",
        "- Use `for check, passed in results.items():`\n",
        "- Set status emoji: \"‚úì\" if passed, else \"‚úó\"\n",
        "- Use ternary operator: `status = \"‚úì\" if passed else \"‚úó\"`\n",
        "- Print: \"{status} {check}\"\n",
        "\n",
        "**Step 4:** Print footer\n",
        "- Print \"=\"*60\n",
        "\n",
        "**Step 5:** Print conditional final message\n",
        "- If `all_passed`:\n",
        "  - Print \"\\nüéâ All checks passed! Your environment is ready.\"\n",
        "  - Print \"\\nNext steps:\"\n",
        "  - Print \"1. Continue to Part 2: Model Download\"\n",
        "- Else:\n",
        "  - Print \"\\n‚ö†Ô∏è  Some checks failed. Please fix the issues above.\"\n",
        "\n",
        "**Step 6:** Print blank line and return result\n",
        "- Print \"\"\n",
        "- Return `all_passed`\n",
        "\n",
        "### ‚úÖ Expected Output (all passing)\n",
        "```\n",
        "============================================================\n",
        "SUMMARY\n",
        "============================================================\n",
        "‚úì Python 3.9+\n",
        "‚úì PyTorch\n",
        "‚úì CUDA\n",
        "‚úì vLLM\n",
        "‚úì HuggingFace Hub\n",
        "============================================================\n",
        "\n",
        "üéâ All checks passed! Your environment is ready.\n",
        "\n",
        "Next steps:\n",
        "1. Continue to Part 2: Model Download\n",
        "\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Using `any()` instead of `all()`**: We need ALL checks to pass\n",
        "2. **Forgetting `.values()`**: `results` is a dict, need to get just the boolean values\n",
        "3. **Wrong number of `=` signs**: Should be exactly 60 for proper formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement print_summary() function\n",
        "def print_summary(results):\n",
        "    \"\"\"Print summary of all checks\"\"\"\n",
        "    # Step 1: Print header\n",
        "    \n",
        "    # Step 2: Check if all passed\n",
        "    \n",
        "    # Step 3: Loop through and print each result\n",
        "    \n",
        "    # Step 4: Print footer\n",
        "    \n",
        "    # Step 5: Print conditional message\n",
        "    \n",
        "    # Step 6: Return result\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Test your function with sample data\n",
        "# sample_results = {\n",
        "#     \"Python 3.9+\": True,\n",
        "#     \"PyTorch\": True,\n",
        "#     \"CUDA\": True,\n",
        "#     \"vLLM\": True,\n",
        "#     \"HuggingFace Hub\": True,\n",
        "# }\n",
        "# print_summary(sample_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Run Complete Environment Verification\n",
        "\n",
        "Now let's put it all together and run a complete environment check!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Run all checks and create summary\n",
        "def run_environment_verification():\n",
        "    \"\"\"Run all environment checks\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Week 1: Environment Verification\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # TODO: Create a dictionary with check names and results\n",
        "    # Call each check function and store the result\n",
        "    results = {\n",
        "        \"Python 3.9+\": check_python_version(),\n",
        "        \"PyTorch\": check_pytorch(),\n",
        "        \"CUDA\": check_cuda(),\n",
        "        \"vLLM\": check_vllm(),\n",
        "        \"HuggingFace Hub\": check_huggingfacehub(),\n",
        "    }\n",
        "    \n",
        "    # TODO: Print summary\n",
        "    success = print_summary(results)\n",
        "    return success\n",
        "\n",
        "# TODO: Run the verification\n",
        "# run_environment_verification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Model Download\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- How to download models from HuggingFace Hub programmatically\n",
        "- How to use `snapshot_download` for large models\n",
        "- How to check if a model is already cached\n",
        "- How to handle download interruptions gracefully\n",
        "- How to work with Path objects for file system operations\n",
        "\n",
        "## üìñ Background\n",
        "The Qwen2.5-7B-Instruct model is approximately 15GB. HuggingFace Hub provides `snapshot_download` which:\n",
        "- Downloads all model files to a local cache\n",
        "- Supports resumable downloads (can be interrupted and continued)\n",
        "- Caches files in `~/.cache/huggingface/hub/` by default\n",
        "- Allows models to be loaded quickly on subsequent uses\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 2.1: `check_model_cache()`\n",
        "\n",
        "### üéØ Goal\n",
        "Check if the Qwen2.5-7B-Instruct model is already cached locally.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Import required modules at the top of the notebook\n",
        "- `from pathlib import Path`\n",
        "- `import os`\n",
        "\n",
        "**Step 2:** Construct the cache directory path\n",
        "- HuggingFace cache location: `~/.cache/huggingface/hub`\n",
        "- Use `Path.home()` to get the user's home directory\n",
        "- Chain with: `/ \".cache\" / \"huggingface\" / \"hub\"`\n",
        "- Store in variable `cache_dir`\n",
        "\n",
        "**Step 3:** Check if cache directory exists\n",
        "- Use `cache_dir.exists()` method\n",
        "- If it doesn't exist, return `False` immediately\n",
        "- No cache directory = no cached models\n",
        "\n",
        "**Step 4:** Look for Qwen model directories\n",
        "- Use `cache_dir.iterdir()` to iterate through all items\n",
        "- For each item, check if \"qwen2.5-7b-instruct\" is in the name (lowercase)\n",
        "- Use `item.name.lower()` to make the search case-insensitive\n",
        "- If found, return `True`\n",
        "\n",
        "**Step 5:** If no Qwen directory found\n",
        "- Return `False`\n",
        "\n",
        "### ‚úÖ Expected Behavior\n",
        "- Returns `True` if a directory containing \"qwen2.5-7b-instruct\" exists in the cache\n",
        "- Returns `False` otherwise\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Case sensitivity**: Model directory names might have different casing, always use `.lower()`\n",
        "2. **Not checking if directory exists first**: `iterdir()` will fail if directory doesn't exist\n",
        "3. **Using string paths instead of Path objects**: Path objects are more robust and cross-platform\n",
        "\n",
        "### üí° Why This Matters\n",
        "Checking the cache before downloading:\n",
        "- Saves time (no need to re-download 15GB)\n",
        "- Saves bandwidth\n",
        "- Provides better user experience (can prompt before re-downloading)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import required modules\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# TODO: Implement check_model_cache() function\n",
        "def check_model_cache():\n",
        "    \"\"\"Check if model is already in cache\"\"\"\n",
        "    # Step 1: Construct cache directory path\n",
        "    \n",
        "    # Step 2: Check if directory exists\n",
        "    \n",
        "    # Step 3: Look for Qwen model directories\n",
        "    \n",
        "    # Step 4: Return result\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Test your function\n",
        "# is_cached = check_model_cache()\n",
        "# print(f\"Model cached: {is_cached}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 2.2: `download_model()`\n",
        "\n",
        "### üéØ Goal\n",
        "Download the Qwen2.5-7B-Instruct model from HuggingFace Hub with proper error handling.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Print a formatted header\n",
        "- Print \"=\"*60\n",
        "- Print \"Week 1: Downloading Qwen2.5-7B-Instruct Model\"\n",
        "- Print \"=\"*60\n",
        "- Print blank line\n",
        "\n",
        "**Step 2:** Try to import snapshot_download\n",
        "- Wrap in try-except to catch ImportError\n",
        "- `from huggingface_hub import snapshot_download`\n",
        "- If ImportError: print error message and return (don't use sys.exit in notebooks!)\n",
        "\n",
        "**Step 3:** Define model name and print info\n",
        "- `model_name = \"Qwen/Qwen2.5-7B-Instruct\"`\n",
        "- Print: \"üì¶ Model: {model_name}\"\n",
        "- Print: \"üìè Expected size: ~15 GB\"\n",
        "- Print: \"üìÅ Cache location: {os.path.expanduser('~/.cache/huggingface')}\"\n",
        "- Print blank line\n",
        "- Print: \"‚è≥ This may take 10-30 minutes depending on your internet speed...\"\n",
        "- Print: \"    You can cancel and run again later - progress is saved.\"\n",
        "- Print blank line\n",
        "\n",
        "**Step 4:** Download the model with proper error handling\n",
        "- Wrap in try-except block with TWO exception types:\n",
        "  1. `KeyboardInterrupt` (user presses Ctrl+C)\n",
        "  2. `Exception` (any other error)\n",
        "\n",
        "**Step 5:** In the try block, call snapshot_download\n",
        "- Use these parameters:\n",
        "  - `repo_id=model_name`\n",
        "  - `cache_dir=None` (use default)\n",
        "  - `resume_download=True` (enable resumable downloads)\n",
        "- Store returned path in `cache_dir` variable\n",
        "\n",
        "**Step 6:** Print success message\n",
        "- Print blank line\n",
        "- Print \"=\"*60\n",
        "- Print \"‚úÖ Model downloaded successfully!\"\n",
        "- Print \"=\"*60\n",
        "- Print: \"üìÅ Cache location: {cache_dir}\"\n",
        "- Print blank line\n",
        "- Print \"Next steps:\"\n",
        "- Print \"1. Continue to Part 3: Baseline Inference\"\n",
        "- Print blank line\n",
        "\n",
        "**Step 7:** Handle KeyboardInterrupt (in except block)\n",
        "- Print: \"\\n\\n‚ö†Ô∏è  Download interrupted by user.\"\n",
        "- Print: \"Progress has been saved. Run this cell again to resume.\"\n",
        "- Return from function (in notebooks, don't use sys.exit)\n",
        "\n",
        "**Step 8:** Handle general exceptions (in except block)\n",
        "- Catch `Exception as e`\n",
        "- Print: \"\\n‚ùå Error downloading model: {e}\"\n",
        "- Print \"\\nTroubleshooting:\"\n",
        "- Print \"1. Check your internet connection\"\n",
        "- Print \"2. Ensure you have enough disk space (~20GB free)\"\n",
        "- Print \"3. Try again - downloads are resumable\"\n",
        "- Print \"4. If issues persist, the model will auto-download on first use\"\n",
        "- Return from function\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Using sys.exit() in notebooks**: This will crash the kernel. Use `return` instead.\n",
        "2. **Not handling KeyboardInterrupt separately**: Users should know their progress is saved.\n",
        "3. **Forgetting resume_download=True**: Downloads should be resumable for large models.\n",
        "4. **Not providing troubleshooting steps**: Users need guidance when errors occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement download_model() function\n",
        "def download_model():\n",
        "    \"\"\"Download Qwen2.5-7B-Instruct model from HuggingFace Hub\"\"\"\n",
        "    # Step 1: Print header\n",
        "    \n",
        "    # Step 2: Try to import snapshot_download\n",
        "    \n",
        "    # Step 3: Define model name and print info\n",
        "    \n",
        "    # Step 4-6: Download with error handling\n",
        "    \n",
        "    # Step 7-8: Handle exceptions\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Uncomment to run the download (WARNING: Large download!)\n",
        "# First, check if model is already cached\n",
        "# if not check_model_cache():\n",
        "#     download_model()\n",
        "# else:\n",
        "#     print(\"‚úÖ Model is already cached!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Baseline Inference\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- How to initialize vLLM's LLM class\n",
        "- How to configure SamplingParams for text generation\n",
        "- How to perform single and batch inference\n",
        "- How to measure inference time and throughput\n",
        "- How to extract and display generated text from outputs\n",
        "\n",
        "## üìñ Background\n",
        "vLLM's `LLM` class provides a high-level interface for:\n",
        "- Loading large language models into GPU memory\n",
        "- Running inference with automatic batching\n",
        "- Efficient memory management with PagedAttention\n",
        "\n",
        "`SamplingParams` controls how text is generated:\n",
        "- `max_tokens`: Maximum number of tokens to generate\n",
        "- `temperature`: Controls randomness (0=deterministic, higher=more random)\n",
        "- `top_p`: Nucleus sampling threshold\n",
        "- `top_k`: Number of top tokens to consider\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 3.1: `run_baseline_inference()`\n",
        "\n",
        "### üéØ Goal\n",
        "Load the Qwen model and perform a simple inference test.\n",
        "\n",
        "### üìã Key Steps\n",
        "\n",
        "**1. Import and Setup**\n",
        "- Import: `from vllm import LLM, SamplingParams`\n",
        "- Import: `import time` for measuring performance\n",
        "- Define model name: `\"Qwen/Qwen2.5-7B-Instruct\"`\n",
        "\n",
        "**2. Load the Model**\n",
        "- Create LLM instance with `trust_remote_code=True`\n",
        "- Measure loading time with `time.time()`\n",
        "- Handle exceptions (GPU memory, CUDA availability, etc.)\n",
        "\n",
        "**3. Configure Sampling**\n",
        "- Create SamplingParams with:\n",
        "  - `max_tokens=20` (generate up to 20 tokens)\n",
        "  - `temperature=0.7` (balanced randomness)\n",
        "  - `top_p=0.9` (nucleus sampling)\n",
        "  - `top_k=50` (top-k sampling)\n",
        "\n",
        "**4. Generate Text**\n",
        "- Define test prompt: `\"Hello, my name is\"`\n",
        "- Call: `outputs = llm.generate([test_prompt], sampling_params)`\n",
        "- **Important**: Pass prompt as a list, even for single prompt!\n",
        "\n",
        "**5. Extract Results**\n",
        "- Access generated text: `outputs[0].outputs[0].text`\n",
        "- Note the structure: outputs[request_index].outputs[completion_index].text\n",
        "\n",
        "**6. Calculate and Display Metrics**\n",
        "- Generation time (seconds)\n",
        "- Tokens generated (approximate with `.split()`)\n",
        "- Tokens per second\n",
        "\n",
        "### ‚úÖ Expected Output\n",
        "```\n",
        "============================================================\n",
        "Week 1: Baseline Inference with vLLM\n",
        "============================================================\n",
        "\n",
        "üì¶ Loading model: Qwen/Qwen2.5-7B-Instruct\n",
        "‚è≥ This may take 30-60 seconds on first load...\n",
        "\n",
        "‚úÖ Model loaded successfully in 45.3 seconds\n",
        "\n",
        "üß™ Running inference test...\n",
        "üìù Prompt: \"Hello, my name is\"\n",
        "\n",
        "‚ú® Generated text:\n",
        "------------------------------------------------------------\n",
        "Hello, my name is Alex and I'm here to help you with any questions\n",
        "------------------------------------------------------------\n",
        "\n",
        "üìä Statistics:\n",
        "   Generation time: 0.89 seconds\n",
        "   Tokens generated: 13\n",
        "   Tokens per second: 14.6\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Not passing prompt as a list**: `generate()` expects a list of prompts\n",
        "2. **Wrong output access path**: It's `outputs[0].outputs[0].text`, not `outputs[0].text`\n",
        "3. **Dividing by zero**: Check generation_time > 0 before calculating tokens/sec\n",
        "4. **Token counting**: Using `.split()` is approximate; actual tokenization may differ\n",
        "5. **Not setting trust_remote_code=True**: Required for Qwen models\n",
        "\n",
        "### üí° Why This Matters\n",
        "- **Baseline measurement**: This gives us a reference point for future optimizations\n",
        "- **Understanding the pipeline**: Loading ‚Üí Tokenization ‚Üí Generation ‚Üí Detokenization\n",
        "- **Performance awareness**: Measuring time helps identify bottlenecks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import necessary modules\n",
        "import time\n",
        "\n",
        "# TODO: Implement run_baseline_inference() function\n",
        "def run_baseline_inference():\n",
        "    \"\"\"Run a basic inference test with Qwen2.5-7B-Instruct\"\"\"\n",
        "    # Step 1: Print header\n",
        "    \n",
        "    # Step 2: Import vLLM (with error handling)\n",
        "    \n",
        "    # Step 3: Load model with timing\n",
        "    \n",
        "    # Step 4: Configure sampling parameters\n",
        "    \n",
        "    # Step 5: Generate text with timing\n",
        "    \n",
        "    # Step 6: Extract and display results\n",
        "    \n",
        "    # Step 7: Print statistics and educational summary\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Run the inference test (WARNING: This loads a 15GB model!)\n",
        "# run_baseline_inference()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Sampling Parameters Experiments\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- How different sampling parameters affect generation quality\n",
        "- How to design systematic experiments\n",
        "- How to collect and structure experimental data\n",
        "- How to analyze performance vs quality trade-offs\n",
        "\n",
        "## üìñ Sampling Parameters Explained:\n",
        "\n",
        "### max_tokens: Output Length\n",
        "- Controls how many tokens to generate\n",
        "- Directly affects generation time (linear relationship)\n",
        "- More tokens = more complete answers but slower\n",
        "\n",
        "### temperature: Randomness\n",
        "- 0.0 = Deterministic (always pick most likely token)\n",
        "- 0.7 = Balanced (default for chat)\n",
        "- 1.0+ = Creative (more random, diverse)\n",
        "- Higher = more variety but potentially less coherent\n",
        "\n",
        "### top_p (nucleus sampling): Probability Mass Threshold\n",
        "- Consider tokens that make up top P% of probability mass\n",
        "- 0.9 = Consider tokens covering 90% probability\n",
        "- Lower = more focused, Higher = more diverse\n",
        "\n",
        "### top_k: Vocabulary Restriction\n",
        "- Only consider the K most likely tokens\n",
        "- 50 = Only look at top 50 tokens\n",
        "- Lower = more conservative, Higher = more variety\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Design\n",
        "\n",
        "You'll implement 4 key functions for experimentation:\n",
        "\n",
        "### 1. `create_results_dir()` - Setup\n",
        "- Create a \"results\" directory for storing experiment data\n",
        "- Use `Path(\"results\").mkdir(exist_ok=True)`\n",
        "\n",
        "### 2. `test_parameter()` - Single Test\n",
        "- Tests one parameter value\n",
        "- Measures generation time and tokens per second\n",
        "- Returns structured results dictionary\n",
        "\n",
        "### 3. `experiment_max_tokens()` - Experiment Runner\n",
        "- Tests multiple values: [10, 25, 50, 100, 200]\n",
        "- Prints formatted analysis table\n",
        "- Shows how generation time scales linearly\n",
        "\n",
        "### 4. `save_all_results()` - Save Results\n",
        "- Save experiment data to JSON with timestamp\n",
        "- Format: `sampling_experiments_YYYYMMDD_HHMMSS.json`\n",
        "- Enables comparison and analysis later\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 4.1: `test_parameter()` - Core Testing Function\n",
        "\n",
        "### üéØ Goal\n",
        "Test a specific parameter value and collect comprehensive results.\n",
        "\n",
        "### üìù Function Signature\n",
        "```python\n",
        "def test_parameter(llm, param_name, param_value, base_params, test_prompt):\n",
        "    \"\"\"\n",
        "    Test a specific parameter value.\n",
        "    \n",
        "    Args:\n",
        "        llm: vLLM LLM instance (already loaded)\n",
        "        param_name: Name of parameter being tested (e.g., 'temperature')\n",
        "        param_value: Value to test (e.g., 0.7)\n",
        "        base_params: Dict of base parameters (others stay constant)\n",
        "        test_prompt: Prompt to use for testing\n",
        "    \n",
        "    Returns:\n",
        "        Dict with results including generated text and metrics\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Create a copy of base_params and update with test value\n",
        "- Use `.copy()` to avoid modifying original\n",
        "- Set: `params_dict[param_name] = param_value`\n",
        "\n",
        "**Step 2:** Create SamplingParams object\n",
        "- `from vllm import SamplingParams`\n",
        "- `sampling_params = SamplingParams(**params_dict)`\n",
        "- The `**` unpacks the dictionary as keyword arguments\n",
        "\n",
        "**Step 3:** Print what's being tested\n",
        "- Print: f\"\\nüìù Testing {param_name}={param_value}\"\n",
        "\n",
        "**Step 4:** Generate text with timing\n",
        "- Record start time: `start = time.time()`\n",
        "- Generate: `outputs = llm.generate([test_prompt], sampling_params)`\n",
        "- Calculate elapsed time: `elapsed = time.time() - start`\n",
        "\n",
        "**Step 5:** Extract results\n",
        "- Get text: `generated_text = outputs[0].outputs[0].text`\n",
        "- Count tokens: `tokens = len(generated_text.split())`\n",
        "\n",
        "**Step 6:** Print preview (truncated if long)\n",
        "- If text > 80 chars: print first 80 chars + \"...\"\n",
        "- Else: print full text\n",
        "- Print time and tokens\n",
        "\n",
        "**Step 7:** Return structured results dictionary\n",
        "```python\n",
        "return {\n",
        "    \"parameter\": param_name,\n",
        "    \"value\": param_value,\n",
        "    \"prompt\": test_prompt,\n",
        "    \"generated_text\": generated_text,\n",
        "    \"generation_time\": elapsed,\n",
        "    \"tokens_generated\": tokens,\n",
        "    \"tokens_per_second\": tokens / elapsed if elapsed > 0 else 0,\n",
        "}\n",
        "```\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Modifying base_params directly**: Always use `.copy()`\n",
        "2. **Division by zero**: Check `elapsed > 0` before calculating tokens/sec\n",
        "3. **Not using `**params_dict`**: This unpacks the dictionary into kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Import required modules\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# TODO: Implement create_results_dir()\n",
        "def create_results_dir():\n",
        "    \"\"\"Create results directory if it doesn't exist\"\"\"\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# TODO: Implement test_parameter()\n",
        "def test_parameter(llm, param_name, param_value, base_params, test_prompt):\n",
        "    \"\"\"Test a specific parameter value\"\"\"\n",
        "    # Step 1: Copy and update parameters\n",
        "    \n",
        "    # Step 2: Create SamplingParams\n",
        "    \n",
        "    # Step 3: Print test info\n",
        "    \n",
        "    # Step 4: Generate with timing\n",
        "    \n",
        "    # Step 5: Extract results\n",
        "    \n",
        "    # Step 6: Print preview\n",
        "    \n",
        "    # Step 7: Return structured results\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Results Comparison & Analysis\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- How to load and parse JSON experiment results\n",
        "- How to create comparison tables\n",
        "- How to extract insights from experimental data\n",
        "- How to make data-driven recommendations\n",
        "\n",
        "## üìñ Key Functions:\n",
        "\n",
        "### 1. `load_latest_results()` - Load Experiment Data\n",
        "- Finds the most recent results file\n",
        "- Uses `glob()` to find matching files\n",
        "- Returns JSON data or None if not found\n",
        "\n",
        "### 2. `compare_max_tokens()` - Analysis Function\n",
        "- Creates formatted comparison table\n",
        "- Calculates average speeds\n",
        "- Shows time scaling relationships\n",
        "\n",
        "### 3. `show_recommendations()` - Best Practices\n",
        "- Provides parameter combinations for different use cases:\n",
        "  - Factual/Technical: temp=0.2, top_p=0.8\n",
        "  - Balanced: temp=0.7, top_p=0.9\n",
        "  - Creative: temp=1.0, top_p=0.95\n",
        "  - Deterministic: temp=0.0, top_p=1.0\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 5.1: `load_latest_results()`\n",
        "\n",
        "### üéØ Goal\n",
        "Load the most recent experiment results from the results directory.\n",
        "\n",
        "### üìã Step-by-Step Instructions\n",
        "\n",
        "**Step 1:** Define results directory path\n",
        "- `results_dir = Path(\"results\")`\n",
        "\n",
        "**Step 2:** Check if directory exists\n",
        "- If not exists:\n",
        "  - Print \"‚ùå No results directory found. Run experiments first!\"\n",
        "  - Return `None`\n",
        "\n",
        "**Step 3:** Find all result files\n",
        "- Use `results_dir.glob(\"sampling_experiments_*.json\")`\n",
        "- Convert to list: `result_files = list(...)`\n",
        "\n",
        "**Step 4:** Check if any files found\n",
        "- If empty list:\n",
        "  - Print error message\n",
        "  - Return `None`\n",
        "\n",
        "**Step 5:** Get the most recent file\n",
        "- Use `max()` with a key function:\n",
        "```python\n",
        "latest_file = max(result_files, key=lambda f: f.stat().st_mtime)\n",
        "```\n",
        "- `st_mtime` is the file modification time\n",
        "\n",
        "**Step 6:** Load and return JSON data\n",
        "- Print: f\"üìÇ Loading: {latest_file.name}\"\n",
        "- Open file: `with open(latest_file) as f:`\n",
        "- Load JSON: `return json.load(f)`\n",
        "\n",
        "### ‚ö†Ô∏è Common Pitfalls\n",
        "1. **Not converting glob to list**: `glob()` returns an iterator\n",
        "2. **Wrong stat attribute**: It's `st_mtime`, not `modified_time`\n",
        "3. **Not handling None returns**: Calling code should check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement load_latest_results()\n",
        "def load_latest_results():\n",
        "    \"\"\"Load the most recent experiment results\"\"\"\n",
        "    # Step 1: Define results directory\n",
        "    \n",
        "    # Step 2: Check if exists\n",
        "    \n",
        "    # Step 3: Find all result files\n",
        "    \n",
        "    # Step 4: Check if any found\n",
        "    \n",
        "    # Step 5: Get most recent\n",
        "    \n",
        "    # Step 6: Load and return\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Custom Parameter Testing\n",
        "\n",
        "## üéì What You'll Learn\n",
        "- How to create reusable testing functions\n",
        "- How to use default parameters effectively\n",
        "- How to provide good user experience\n",
        "\n",
        "## üìñ Interactive Testing\n",
        "\n",
        "The `test_generation()` function allows quick testing of any parameter combination:\n",
        "\n",
        "```python\n",
        "# Test with defaults\n",
        "result = test_generation(\"Explain AI\")\n",
        "\n",
        "# Test with custom parameters\n",
        "result = test_generation(\n",
        "    \"Explain AI\", \n",
        "    temperature=0.0,  # Deterministic\n",
        "    max_tokens=100\n",
        ")\n",
        "```\n",
        "\n",
        "### Default Parameters Make Functions User-Friendly:\n",
        "- Can call with just a prompt\n",
        "- Or override specific params\n",
        "- Users don't need to specify everything\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 6.1: `test_generation()` - Quick Testing Tool\n",
        "\n",
        "### üéØ Goal\n",
        "Test text generation with any combination of parameters, with sensible defaults.\n",
        "\n",
        "### üìù Function Signature\n",
        "```python\n",
        "def test_generation(prompt, max_tokens=50, temperature=0.7, top_p=0.9, top_k=50):\n",
        "    \"\"\"\n",
        "    Test text generation with specific parameters.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input prompt (required)\n",
        "        max_tokens: Maximum tokens to generate (default: 50)\n",
        "        temperature: Sampling temperature 0.0-2.0 (default: 0.7)\n",
        "        top_p: Nucleus sampling 0.0-1.0 (default: 0.9)\n",
        "        top_k: Top-k sampling (default: 50)\n",
        "    \n",
        "    Returns:\n",
        "        Dict with results and metrics\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "### üìã Implementation Steps\n",
        "\n",
        "**1. Load Model**\n",
        "- Import vLLM components\n",
        "- Load Qwen model with trust_remote_code=True\n",
        "\n",
        "**2. Configure and Generate**\n",
        "- Create SamplingParams with all provided parameters\n",
        "- Print configuration being used\n",
        "- Generate text with timing\n",
        "\n",
        "**3. Display Results**\n",
        "- Print generated text in a formatted box\n",
        "- Show all metrics (time, tokens, speed)\n",
        "\n",
        "**4. Return Structured Result**\n",
        "- Include timestamp, prompt, parameters, generated text, metrics\n",
        "- Return as dictionary for potential further use\n",
        "\n",
        "### üí° Why Default Parameters?\n",
        "- Easier to use: `test_generation(\"Hello\")` just works\n",
        "- Flexible: Can override any parameter\n",
        "- Self-documenting: Defaults show recommended values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement test_generation()\n",
        "def test_generation(prompt, max_tokens=50, temperature=0.7, top_p=0.9, top_k=50):\n",
        "    \"\"\"Test text generation with specific parameters\"\"\"\n",
        "    # Step 1: Load model\n",
        "    \n",
        "    # Step 2: Configure sampling\n",
        "    \n",
        "    # Step 3: Generate with timing\n",
        "    \n",
        "    # Step 4: Display results\n",
        "    \n",
        "    # Step 5: Return structured result\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Test with different parameter combinations\n",
        "# Example 1: Deterministic\n",
        "# result = test_generation(\"Explain machine learning:\", temperature=0.0)\n",
        "\n",
        "# Example 2: Creative\n",
        "# result = test_generation(\"Write a story:\", temperature=1.2, max_tokens=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéâ Congratulations!\n",
        "\n",
        "## What You've Accomplished\n",
        "\n",
        "You've successfully implemented a complete Week 1 learning system for vLLM and Qwen2.5! You now understand:\n",
        "\n",
        "### ‚úÖ Environment Setup\n",
        "- How to verify Python, PyTorch, CUDA, and library installations\n",
        "- How to check GPU availability programmatically\n",
        "- How to create comprehensive validation scripts\n",
        "\n",
        "### ‚úÖ Model Management\n",
        "- How to download large models from HuggingFace Hub\n",
        "- How to check local cache\n",
        "- How to handle download interruptions and resumption\n",
        "\n",
        "### ‚úÖ Inference Basics\n",
        "- How to initialize vLLM's LLM class\n",
        "- How to configure SamplingParams\n",
        "- How to perform single and batch inference\n",
        "- How to measure inference performance\n",
        "\n",
        "### ‚úÖ Parameter Experimentation\n",
        "- How max_tokens affects output length and speed\n",
        "- How temperature controls randomness and creativity\n",
        "- How top_p and top_k influence diversity\n",
        "- How to design systematic experiments\n",
        "\n",
        "### ‚úÖ Data Analysis\n",
        "- How to save and load experiment results\n",
        "- How to compare parameter effects\n",
        "- How to derive insights from experimental data\n",
        "- How to make informed recommendations\n",
        "\n",
        "### ‚úÖ Software Engineering Skills\n",
        "- Error handling and user feedback\n",
        "- File system operations with pathlib\n",
        "- JSON serialization and deserialization\n",
        "- Creating reusable, well-documented functions\n",
        "\n",
        "---\n",
        "\n",
        "## üîú Next Steps\n",
        "\n",
        "1. **Experiment Further**: Try your own prompts and parameter combinations\n",
        "2. **Compare Results**: Run the same prompt with different parameters\n",
        "3. **Document Findings**: Keep notes on what works well for your use cases\n",
        "4. **Week 2**: Move on to performance profiling and optimization!\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Quick Reference: Sampling Parameters\n",
        "\n",
        "```python\n",
        "# Factual/Technical\n",
        "SamplingParams(max_tokens=100, temperature=0.2, top_p=0.8, top_k=40)\n",
        "\n",
        "# Balanced/Chat\n",
        "SamplingParams(max_tokens=150, temperature=0.7, top_p=0.9, top_k=50)\n",
        "\n",
        "# Creative\n",
        "SamplingParams(max_tokens=200, temperature=1.0, top_p=0.95, top_k=100)\n",
        "\n",
        "# Deterministic (for testing/reproducibility)\n",
        "SamplingParams(max_tokens=100, temperature=0.0, top_p=1.0, top_k=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üôè Great Work!\n",
        "\n",
        "You've learned by doing - the best way to truly understand a technology. The code you've written is now yours to modify, extend, and apply to your own projects.\n",
        "\n",
        "**Key Takeaway**: Understanding how to tune sampling parameters is crucial for getting the best results from LLMs. Different tasks require different settings!\n",
        "\n",
        "Keep experimenting and happy coding! üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
